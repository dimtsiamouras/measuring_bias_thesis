{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOfWh9IPgHBv"
      },
      "source": [
        "https://github.com/dbouchabou/Fully-Convolutional-Network-Smart-Homes/tree/master"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eZvoGlUiADu",
        "outputId": "8f2c66b2-b631-4e76-f272-bdbccf15769a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive' , force_remount = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65A1BVG_f5_A"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdX3CCKeMz2u"
      },
      "source": [
        "### MHEALTH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Jg55Tm3M680",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b12431cf-2911-4c7d-9388-a06163f04918"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/MHEALTHDATASET\n"
          ]
        }
      ],
      "source": [
        "cd \"/content/drive/MyDrive/MHEALTHDATASET/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKzutIpEM67K"
      },
      "outputs": [],
      "source": [
        "df_mhealth = pd.read_pickle('mhealth_for_lstm.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bySOOtMNf_Z"
      },
      "source": [
        "#### LOSO split here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogtbOxlmNjTZ"
      },
      "outputs": [],
      "source": [
        "train_data = df_mhealth[(df_mhealth[\"subject\"] != 'subject4') & (df_mhealth[\"subject\"] != 'subject7')]\n",
        "test_data = df_mhealth[(df_mhealth[\"subject\"] == 'subject4')]\n",
        "val_data = df_mhealth[(df_mhealth[\"subject\"] == 'subject7')]\n",
        "#train_data = df_mhealth[(df_mhealth[\"subject\"] != 'subject4')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hT68wd_RHE2q"
      },
      "outputs": [],
      "source": [
        "train_data = train_data.sort_values(by=['subject', 'activity'])\n",
        "test_data = test_data.sort_values(by=['subject', 'activity'])\n",
        "val_data = val_data.sort_values(by=['subject', 'activity'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjNKmPNMM6yr"
      },
      "outputs": [],
      "source": [
        "def segment_activities(df):\n",
        "    activitiesSeq = []\n",
        "\n",
        "    ponentialIndex = df.activity.ne(df.activity.shift())\n",
        "\n",
        "    ii = np.where(ponentialIndex == True)[0]\n",
        "\n",
        "    for i,end in enumerate(ii):\n",
        "        if i > 0 :\n",
        "\n",
        "          dftmp = df[ii[i-1]:end]\n",
        "          activitiesSeq.append(dftmp)\n",
        "    return activitiesSeq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GrNrO1gOLv9"
      },
      "outputs": [],
      "source": [
        "train_activitySequences = segment_activities(train_data)\n",
        "val_activitySequences = segment_activities(val_data)\n",
        "test_activitySequences = segment_activities(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmZyiAtBM6wb"
      },
      "outputs": [],
      "source": [
        "def generate_sentence(df2):\n",
        "    sentence = \"\"\n",
        "\n",
        "    #columns for which to retain values\n",
        "    columns_of_interest = ['acc_ch_x', 'acc_ch_y', 'acc_ch_z', 'ecg_sig_1', 'ecg_sig_2',\n",
        "       'acc_la_x', 'acc_la_y', 'acc_la_z', 'gyr_la_x', 'gyr_la_y', 'gyr_la_z',\n",
        "       'mag_la_x', 'mag_la_y', 'mag_la_z', 'acc_rw_x', 'acc_rw_y', 'acc_rw_z',\n",
        "       'gyr_rw_x', 'gyr_rw_y', 'gyr_rw_z', 'mag_rw_x', 'mag_rw_y', 'mag_rw_z',\n",
        "       'activity', 'subject']\n",
        "\n",
        "    #iterate over columns\n",
        "    for column in columns_of_interest:\n",
        "        #column value\n",
        "        value = df2[column].values[0]  # Directly access the single value in the column\n",
        "\n",
        "        #column name and val\n",
        "        sentence += \"{}{}\".format(column, value)\n",
        "\n",
        "        #space if not last col\n",
        "        if column != columns_of_interest[-1]:\n",
        "            sentence += \" \"\n",
        "\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juv2h-yZM6tM"
      },
      "outputs": [],
      "source": [
        "def sequencesToSentences(activitySequences):\n",
        "\tsentences = []\n",
        "\tlabel_sentences = []\n",
        "\n",
        "\tfor i in range(len(activitySequences)):\n",
        "\n",
        "\t\tsentence = generate_sentence(activitySequences[i])\n",
        "\n",
        "\t\tsentences.append(sentence)\n",
        "\t\tlabel_sentences.append(activitySequences[i].activity.values[0])\n",
        "\n",
        "\treturn sentences, label_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4qEj19RO58n"
      },
      "outputs": [],
      "source": [
        "train_sentences, train_label_sentences = sequencesToSentences(train_activitySequences)\n",
        "val_sentences, val_label_sentences = sequencesToSentences(val_activitySequences)\n",
        "test_sentences, test_label_sentences = sequencesToSentences(test_activitySequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFBg_m9zNFdC"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(filters='!\"#$%&()*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "combined_sentences = train_sentences + val_sentences + test_sentences\n",
        "tokenizer.fit_on_texts(combined_sentences)\n",
        "\n",
        "# Tokenize train, validation, and test sets\n",
        "train_indexed_sentences = tokenizer.texts_to_sequences(train_sentences)\n",
        "val_indexed_sentences = tokenizer.texts_to_sequences(val_sentences)\n",
        "test_indexed_sentences = tokenizer.texts_to_sequences(test_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1xOWWi1NJmk"
      },
      "source": [
        "#### sliding window data segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xzjs3W1oNFXg"
      },
      "outputs": [],
      "source": [
        "def slidingWindow(sequence,winSize,step=1):\n",
        "\n",
        "    try: it = iter(sequence)\n",
        "    except TypeError:\n",
        "        raise Exception(\"**ERROR** sequence must be iterable.\")\n",
        "    if not ((type(winSize) == type(0)) and (type(step) == type(0))):\n",
        "        raise Exception(\"**ERROR** type(winSize) and type(step) must be int.\")\n",
        "    if step > winSize:\n",
        "        raise Exception(\"**ERROR** step must not be larger than winSize.\")\n",
        "\n",
        "    numOfChunks = int(((len(sequence)-winSize)/step)+1)\n",
        "\n",
        "    if winSize > len(sequence):\n",
        "        yield sequence[0:len(sequence)]\n",
        "    else:\n",
        "        for i in range(0,numOfChunks*step,step):\n",
        "            yield sequence[i:i+winSize]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRxY2Xl4NFTt"
      },
      "outputs": [],
      "source": [
        "X_train_windowed = []\n",
        "Y_train_windowed = []\n",
        "winSize = 50\n",
        "step = 1\n",
        "\n",
        "for i,s in enumerate(train_indexed_sentences):\n",
        "\tchunks = slidingWindow(s,winSize,step)\n",
        "\tfor chunk in chunks:\n",
        "\t\tX_train_windowed.append(chunk)\n",
        "\t\tY_train_windowed.append(train_label_sentences[i])\n",
        "\n",
        "X_val_windowed = []\n",
        "Y_val_windowed = []\n",
        "winSize = 50\n",
        "step = 1\n",
        "\n",
        "for i,s in enumerate(val_indexed_sentences):\n",
        "\tchunks = slidingWindow(s,winSize,step)\n",
        "\tfor chunk in chunks:\n",
        "\t\tX_val_windowed.append(chunk)\n",
        "\t\tY_val_windowed.append(val_label_sentences[i])\n",
        "\n",
        "\n",
        "X_test_windowed = []\n",
        "Y_test_windowed = []\n",
        "winSize = 50\n",
        "step = 1\n",
        "\n",
        "for i,s in enumerate(test_indexed_sentences):\n",
        "\tchunks = slidingWindow(s,winSize,step)\n",
        "\tfor chunk in chunks:\n",
        "\t\tX_test_windowed.append(chunk)\n",
        "\t\tY_test_windowed.append(test_label_sentences[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rPgEw2ENFRi"
      },
      "outputs": [],
      "source": [
        "train_padded_windows = pad_sequences(X_train_windowed)\n",
        "test_padded_windows = pad_sequences(X_test_windowed)\n",
        "val_padded_windows = pad_sequences(X_val_windowed)\n",
        "\n",
        "Y_train_windowed = np.array(Y_train_windowed)\n",
        "Y_test_windowed = np.array(Y_test_windowed)\n",
        "Y_val_windowed = np.array(Y_val_windowed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWI_9TXu2BnC"
      },
      "outputs": [],
      "source": [
        "x_train = train_padded_windows\n",
        "y_train = Y_train_windowed\n",
        "x_test = test_padded_windows\n",
        "y_test = Y_test_windowed\n",
        "x_val = val_padded_windows\n",
        "y_val = Y_val_windowed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBpivb_RNFCU",
        "outputId": "d2fdc33a-f927-4616-a78a-5df320c6fd6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Jensen-Shannon divergence: 0.17813294591044218\n"
          ]
        }
      ],
      "source": [
        "from scipy.stats import entropy\n",
        "\n",
        "js_divergences = np.zeros(x_train.shape[1])\n",
        "for i in range(x_train.shape[1]):\n",
        "    # Compute histograms with the same bins for both X_train and X_test\n",
        "    bins = max(len(np.unique(x_train[:, i])), len(np.unique(x_test[:, i])))\n",
        "    p, _ = np.histogram(x_train[:, i], bins=bins, density=True)\n",
        "    q, _ = np.histogram(x_test[:, i], bins=bins, density=True)\n",
        "    m = 0.5 * (p + q)\n",
        "    js_divergences[i] = 0.5 * (entropy(p, m) + entropy(q, m))\n",
        "\n",
        "avg_js_divergence = np.mean(js_divergences)\n",
        "print(\"Average Jensen-Shannon divergence:\", avg_js_divergence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGJRmW8WM2F3"
      },
      "source": [
        "### PAMAP2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s562sAmxkAZd",
        "outputId": "cc1d95da-a273-41d7-8ed1-a72240d628ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/PAMAP2_Dataset\n"
          ]
        }
      ],
      "source": [
        "cd \"/content/drive/MyDrive/PAMAP2_Dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhYNvGgOiLrC"
      },
      "outputs": [],
      "source": [
        "data = pd.read_pickle('pamap2_for_lstm.pkl')\n",
        "data = data[data['id'] != 109]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4ELz0YdlR_c"
      },
      "outputs": [],
      "source": [
        "activity_counts = data.groupby('id')['activity_id'].value_counts()\n",
        "activity_counts_df = activity_counts.reset_index(name='count')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0M8CLtpQVWzx"
      },
      "outputs": [],
      "source": [
        "test_data = data[(data[\"id\"] == 107)]\n",
        "train_data = data[(data[\"id\"] != 107) & (data[\"id\"] != 104)]\n",
        "val_data = data[(data[\"id\"] == 104)]\n",
        "\n",
        "train_activity_id = train_data['activity_id']\n",
        "train_sub_id = train_data['id']\n",
        "test_activity_id = test_data['activity_id']\n",
        "test_sub_id = test_data['id']\n",
        "val_activity_id = val_data['activity_id']\n",
        "val_sub_id = val_data['id']\n",
        "\n",
        "train_df_without_id = train_data.drop(columns=['activity_id', 'id'])\n",
        "test_df_without_id = test_data.drop(columns=['activity_id', 'id'])\n",
        "val_df_without_id = val_data.drop(columns=['activity_id', 'id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ddx4ipTmVqxu"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "numerical_columns = train_df_without_id.select_dtypes(include=['number']).columns\n",
        "\n",
        "scaler = StandardScaler()\n",
        "train_scaled = scaler.fit_transform(train_df_without_id[numerical_columns])\n",
        "train_scaled_df = pd.DataFrame(train_scaled, columns=numerical_columns, index=train_df_without_id.index)\n",
        "\n",
        "test_scaled = scaler.transform(test_df_without_id[numerical_columns])\n",
        "test_scaled_df = pd.DataFrame(test_scaled, columns=numerical_columns, index=test_df_without_id.index)\n",
        "\n",
        "val_scaled = scaler.transform(val_df_without_id[numerical_columns])\n",
        "val_scaled_df = pd.DataFrame(val_scaled, columns=numerical_columns, index=val_df_without_id.index)\n",
        "\n",
        "train_scaled_df = pd.concat([train_activity_id, train_sub_id, train_scaled_df], axis=1)\n",
        "test_scaled_df = pd.concat([test_activity_id, test_sub_id, test_scaled_df], axis=1)\n",
        "val_scaled_df = pd.concat([val_activity_id, val_sub_id, val_scaled_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKusxSsdNK5H"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import LeaveOneGroupOut\n",
        "X = data.drop(columns=['id', 'activity_id'])  # Assuming 'activity_id' and 'id' are dropped as features\n",
        "y = data['activity_id']\n",
        "\n",
        "groups = data['id']\n",
        "logo = LeaveOneGroupOut()\n",
        "\n",
        "# Iterate through the train and test indices generated by LeaveOneGroupOut\n",
        "for train_index, test_index in logo.split(X, y, groups):\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-LCTycdiLkp"
      },
      "outputs": [],
      "source": [
        "def segment_activities(df):\n",
        "    activitiesSeq = []\n",
        "\n",
        "    ponentialIndex = df.activity_id.ne(df.activity_id.shift())\n",
        "\n",
        "    ii = np.where(ponentialIndex == True)[0]\n",
        "\n",
        "    for i,end in enumerate(ii):\n",
        "        if i > 0 :\n",
        "\n",
        "          dftmp = df[ii[i-1]:end]\n",
        "          activitiesSeq.append(dftmp)\n",
        "    return activitiesSeq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BB2VudGYqEzt"
      },
      "outputs": [],
      "source": [
        "train_activitySequences = segment_activities(train_data)\n",
        "val_activitySequences = segment_activities(val_data)\n",
        "test_activitySequences = segment_activities(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMtD9IEiAlmG"
      },
      "source": [
        "#### with my columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SzDwoJSAkCb"
      },
      "outputs": [],
      "source": [
        "def generate_sentence(df2):\n",
        "    sentence = \"\"\n",
        "\n",
        "    # Define the column names you want to extract values from\n",
        "    columns_of_interest = ['heart_rate', 'hand_temperature', 'hand_3D_acceleration_16_x', 'hand_3D_acceleration_16_y',\n",
        "                           'hand_3D_acceleration_16_z', 'hand_3D_acceleration_6_x', 'hand_3D_acceleration_6_y',\n",
        "                           'hand_3D_acceleration_6_z', 'hand_3D_gyroscope_x', 'hand_3D_gyroscope_y',\n",
        "                           'hand_3D_gyroscope_z', 'hand_3D_magnetometer_x', 'hand_3D_magnetometer_y',\n",
        "                           'hand_3D_magnetometer_z', 'chest_temperature', 'chest_3D_acceleration_16_x',\n",
        "                           'chest_3D_acceleration_16_y', 'chest_3D_acceleration_16_z', 'chest_3D_acceleration_6_x',\n",
        "                           'chest_3D_acceleration_6_y', 'chest_3D_acceleration_6_z', 'chest_3D_gyroscope_x',\n",
        "                           'chest_3D_gyroscope_y', 'chest_3D_gyroscope_z', 'chest_3D_magnetometer_x',\n",
        "                           'chest_3D_magnetometer_y', 'chest_3D_magnetometer_z', 'ankle_temperature',\n",
        "                           'ankle_3D_acceleration_16_x', 'ankle_3D_acceleration_16_y', 'ankle_3D_acceleration_16_z',\n",
        "                           'ankle_3D_acceleration_6_x', 'ankle_3D_acceleration_6_y', 'ankle_3D_acceleration_6_z',\n",
        "                           'ankle_3D_gyroscope_x', 'ankle_3D_gyroscope_y', 'ankle_3D_gyroscope_z',\n",
        "                           'ankle_3D_magnetometer_x', 'ankle_3D_magnetometer_y', 'ankle_3D_magnetometer_z']\n",
        "\n",
        "    # Iterate over the columns of interest\n",
        "    for column in columns_of_interest:\n",
        "        # Get the value for the current column\n",
        "        value = df2[column].values[0]  # Directly access the single value in the column\n",
        "\n",
        "        # Add column name and value to the sentence\n",
        "        sentence += \"{}{}\".format(column, value)\n",
        "\n",
        "        # Add a space if it's not the last column\n",
        "        if column != columns_of_interest[-1]:\n",
        "            sentence += \" \"\n",
        "\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDZWx6V0Aj6k"
      },
      "outputs": [],
      "source": [
        "def sequencesToSentences(activitySequences):\n",
        "\tsentences = []\n",
        "\tlabel_sentences = []\n",
        "\n",
        "\tfor i in range(len(activitySequences)):\n",
        "\n",
        "\t\tsentence = generate_sentence(activitySequences[i])\n",
        "\n",
        "\t\tsentences.append(sentence)\n",
        "\t\tlabel_sentences.append(activitySequences[i].activity_id.values[0])\n",
        "\n",
        "\treturn sentences, label_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaInQMtIAo_a"
      },
      "outputs": [],
      "source": [
        "train_sentences, train_label_sentences = sequencesToSentences(train_activitySequences)\n",
        "val_sentences, val_label_sentences = sequencesToSentences(val_activitySequences)\n",
        "test_sentences, test_label_sentences = sequencesToSentences(test_activitySequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXs2oRCAU6VU"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(filters='!\"#$%&()*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "combined_sentences = train_sentences + val_sentences + test_sentences\n",
        "tokenizer.fit_on_texts(combined_sentences)\n",
        "\n",
        "#tokenize train, validation, and test sets\n",
        "train_indexed_sentences = tokenizer.texts_to_sequences(train_sentences)\n",
        "val_indexed_sentences = tokenizer.texts_to_sequences(val_sentences)\n",
        "test_indexed_sentences = tokenizer.texts_to_sequences(test_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqWPQ1VEB0Hh"
      },
      "outputs": [],
      "source": [
        "def slidingWindow(sequence,winSize,step=1):\n",
        "\n",
        "    try: it = iter(sequence)\n",
        "    except TypeError:\n",
        "        raise Exception(\"**ERROR** sequence must be iterable.\")\n",
        "    if not ((type(winSize) == type(0)) and (type(step) == type(0))):\n",
        "        raise Exception(\"**ERROR** type(winSize) and type(step) must be int.\")\n",
        "    if step > winSize:\n",
        "        raise Exception(\"**ERROR** step must not be larger than winSize.\")\n",
        "\n",
        "    numOfChunks = int(((len(sequence)-winSize)/step)+1)\n",
        "\n",
        "    # Do the work\n",
        "    if winSize > len(sequence):\n",
        "        yield sequence[0:len(sequence)]\n",
        "    else:\n",
        "        for i in range(0,numOfChunks*step,step):\n",
        "            yield sequence[i:i+winSize]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZD75iENMB0-V"
      },
      "outputs": [],
      "source": [
        "X_train_windowed = []\n",
        "Y_train_windowed = []\n",
        "winSize = 50\n",
        "step = 1\n",
        "\n",
        "for i,s in enumerate(train_indexed_sentences):\n",
        "\tchunks = slidingWindow(s,winSize,step)\n",
        "\tfor chunk in chunks:\n",
        "\t\tX_train_windowed.append(chunk)\n",
        "\t\tY_train_windowed.append(train_label_sentences[i])\n",
        "\n",
        "X_val_windowed = []\n",
        "Y_val_windowed = []\n",
        "winSize = 50\n",
        "step = 1\n",
        "\n",
        "for i,s in enumerate(val_indexed_sentences):\n",
        "\tchunks = slidingWindow(s,winSize,step)\n",
        "\tfor chunk in chunks:\n",
        "\t\tX_val_windowed.append(chunk)\n",
        "\t\tY_val_windowed.append(val_label_sentences[i])\n",
        "\n",
        "\n",
        "X_test_windowed = []\n",
        "Y_test_windowed = []\n",
        "winSize = 50\n",
        "step = 1\n",
        "\n",
        "for i,s in enumerate(test_indexed_sentences):\n",
        "\tchunks = slidingWindow(s,winSize,step)\n",
        "\tfor chunk in chunks:\n",
        "\t\tX_test_windowed.append(chunk)\n",
        "\t\tY_test_windowed.append(test_label_sentences[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCeoXDmhChpH"
      },
      "outputs": [],
      "source": [
        "train_padded_windows = pad_sequences(X_train_windowed)\n",
        "test_padded_windows = pad_sequences(X_test_windowed)\n",
        "val_padded_windows = pad_sequences(X_val_windowed)\n",
        "\n",
        "Y_train_windowed = np.array(Y_train_windowed)\n",
        "Y_test_windowed = np.array(Y_test_windowed)\n",
        "Y_val_windowed = np.array(Y_val_windowed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = train_padded_windows\n",
        "y_train = Y_train_windowed\n",
        "x_test = test_padded_windows\n",
        "y_test = Y_test_windowed\n",
        "x_val = val_padded_windows\n",
        "y_val = Y_val_windowed"
      ],
      "metadata": {
        "id": "Y0Wsw4p-Mp9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBEaTL79xiM0",
        "outputId": "a84825dc-6895-4c83-e5c1-a99f49045855"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Jensen-Shannon divergence: 0.15987888346998558\n"
          ]
        }
      ],
      "source": [
        "from scipy.stats import entropy\n",
        "\n",
        "js_divergences = np.zeros(x_train.shape[1])\n",
        "for i in range(x_train.shape[1]):\n",
        "    # Compute histograms with the same bins for both X_train and X_test\n",
        "    bins = max(len(np.unique(x_train[:, i])), len(np.unique(x_test[:, i])))\n",
        "    p, _ = np.histogram(x_train[:, i], bins=bins, density=True)\n",
        "    q, _ = np.histogram(x_test[:, i], bins=bins, density=True)\n",
        "    m = 0.5 * (p + q)\n",
        "    js_divergences[i] = 0.5 * (entropy(p, m) + entropy(q, m))\n",
        "\n",
        "avg_js_divergence = np.mean(js_divergences)\n",
        "print(\"Average Jensen-Shannon divergence:\", avg_js_divergence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wzD1GFmmxbb"
      },
      "source": [
        "#### Split on subjects:\n",
        "  subjects 105 for val\n",
        "  106 for test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVwm-Ob3pUIQ"
      },
      "outputs": [],
      "source": [
        "all_data = pd.concat(activitySequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tn4BqQGmvV7"
      },
      "outputs": [],
      "source": [
        "unique_subject_ids = all_data['id'].unique()\n",
        "\n",
        "train_data = []\n",
        "val_data = []\n",
        "test_data = []\n",
        "\n",
        "for subject_id in unique_subject_ids:\n",
        "    subject_data = all_data[all_data['id'] == subject_id]\n",
        "\n",
        "    if subject_id == 105:\n",
        "        val_data.append(subject_data)\n",
        "    elif subject_id == 106:\n",
        "        test_data.append(subject_data)\n",
        "    else:\n",
        "        train_data.append(subject_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RE3KhnKbpl1t"
      },
      "outputs": [],
      "source": [
        "train_data = pd.concat(train_data)\n",
        "val_data = pd.concat(val_data)\n",
        "test_data = pd.concat(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30EZBsZiplz9",
        "outputId": "d922fa40-2e3d-448d-f25b-84854e880169"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[105]\n"
          ]
        }
      ],
      "source": [
        "unique_train_ids = val_data['id'].unique()\n",
        "print(unique_train_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aV18MEXDplyB"
      },
      "outputs": [],
      "source": [
        "def generate_sentence(df2):\n",
        "    sentence = \"\"\n",
        "\n",
        "    # Define the column names you want to extract values from\n",
        "    columns_of_interest = ['heart_rate', 'hand_temperature', 'hand_3D_acceleration_16_x', 'hand_3D_acceleration_16_y',\n",
        "                           'hand_3D_acceleration_16_z', 'hand_3D_acceleration_6_x', 'hand_3D_acceleration_6_y',\n",
        "                           'hand_3D_acceleration_6_z', 'hand_3D_gyroscope_x', 'hand_3D_gyroscope_y',\n",
        "                           'hand_3D_gyroscope_z', 'hand_3D_magnetometer_x', 'hand_3D_magnetometer_y',\n",
        "                           'hand_3D_magnetometer_z', 'chest_temperature', 'chest_3D_acceleration_16_x',\n",
        "                           'chest_3D_acceleration_16_y', 'chest_3D_acceleration_16_z', 'chest_3D_acceleration_6_x',\n",
        "                           'chest_3D_acceleration_6_y', 'chest_3D_acceleration_6_z', 'chest_3D_gyroscope_x',\n",
        "                           'chest_3D_gyroscope_y', 'chest_3D_gyroscope_z', 'chest_3D_magnetometer_x',\n",
        "                           'chest_3D_magnetometer_y', 'chest_3D_magnetometer_z', 'ankle_temperature',\n",
        "                           'ankle_3D_acceleration_16_x', 'ankle_3D_acceleration_16_y', 'ankle_3D_acceleration_16_z',\n",
        "                           'ankle_3D_acceleration_6_x', 'ankle_3D_acceleration_6_y', 'ankle_3D_acceleration_6_z',\n",
        "                           'ankle_3D_gyroscope_x', 'ankle_3D_gyroscope_y', 'ankle_3D_gyroscope_z',\n",
        "                           'ankle_3D_magnetometer_x', 'ankle_3D_magnetometer_y', 'ankle_3D_magnetometer_z']\n",
        "\n",
        "    # Iterate over the columns of interest\n",
        "    for column in columns_of_interest:\n",
        "        # Get the value for the current column\n",
        "        value = df2[column]  # Directly access the single value in the column\n",
        "\n",
        "        # Add column name and value to the sentence\n",
        "        sentence += \"{}{}\".format(column, value)\n",
        "\n",
        "        # Add a space if it's not the last column\n",
        "        if column != columns_of_interest[-1]:\n",
        "            sentence += \" \"\n",
        "\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZEt6O6IoRtR"
      },
      "outputs": [],
      "source": [
        "def sequencesToSentences(activitySequences):\n",
        "    sentences = []\n",
        "    label_sentences = []\n",
        "\n",
        "    for _, row in activitySequences.iterrows():\n",
        "        sentence = generate_sentence(row)\n",
        "        sentences.append(sentence)\n",
        "        label_sentences.append(row['activity_id'])\n",
        "\n",
        "    return sentences, label_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CD235wbyqyJ4"
      },
      "outputs": [],
      "source": [
        "train_sentences, train_label_sentences = sequencesToSentences(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "disvXs5EvBlJ"
      },
      "outputs": [],
      "source": [
        "test_sentences, test_label_sentences = sequencesToSentences(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FB95FFzwvBdV"
      },
      "outputs": [],
      "source": [
        "val_sentences, val_label_sentences = sequencesToSentences(val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5gI_FUUv6rx",
        "outputId": "9b14ffa9-c03a-4875-ceaa-600942348533"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "272442"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(val_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9RBqRSH4JVz"
      },
      "outputs": [],
      "source": [
        "train_sentences_array = np.array(train_sentences)\n",
        "train_label_sentences_array = np.array(train_label_sentences)\n",
        "\n",
        "# Define the file paths\n",
        "sentences_file = \"train_sentences.txt\"\n",
        "labels_file = \"train_label_sentences.txt\"\n",
        "\n",
        "# Save the sentences and labels to separate text files\n",
        "np.savetxt(sentences_file, train_sentences_array, fmt=\"%s\")\n",
        "np.savetxt(labels_file, train_label_sentences_array, fmt=\"%s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfLTgkflrXR4"
      },
      "outputs": [],
      "source": [
        "tokenizer1 = Tokenizer(filters='!\"#$%&()*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "tokenizer1.fit_on_texts(train_sentences)\n",
        "word_index1 = tokenizer1.word_index\n",
        "train_indexed_sentences = tokenizer1.texts_to_sequences(train_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QIVbkLivT7a"
      },
      "outputs": [],
      "source": [
        "tokenizer2 = Tokenizer(filters='!\"#$%&()*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "tokenizer2.fit_on_texts(test_sentences)\n",
        "word_index2 = tokenizer2.word_index\n",
        "test_indexed_sentences = tokenizer2.texts_to_sequences(test_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_0QQoljvT5o"
      },
      "outputs": [],
      "source": [
        "tokenizer3 = Tokenizer(filters='!\"#$%&()*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "tokenizer3.fit_on_texts(val_sentences)\n",
        "word_index3 = tokenizer3.word_index\n",
        "val_indexed_sentences = tokenizer3.texts_to_sequences(val_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anuL_g-JpzYp"
      },
      "source": [
        "#### original"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPx829r-qcn7"
      },
      "outputs": [],
      "source": [
        "def generate_sentence(df2):\n",
        "\n",
        "    sentence = \"\"\n",
        "\n",
        "    val = \"\"\n",
        "\n",
        "    #extract sensors list\n",
        "    sensors = df2.sensor.values\n",
        "\n",
        "    values = df2.value.values\n",
        "\n",
        "    #iterate on sensors list\n",
        "    for i in range(len(sensors)):\n",
        "\n",
        "        val = values[i]\n",
        "\n",
        "        if i == len(sensors) - 1:\n",
        "            sentence += \"{}{}\".format(sensors[i],val)\n",
        "        else:\n",
        "            sentence += \"{}{} \".format(sensors[i],val)\n",
        "\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-MCQ3kEp14o"
      },
      "source": [
        "#### with my columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YB0mTFO5p1od"
      },
      "outputs": [],
      "source": [
        "def generate_sentence(df2):\n",
        "    sentence = \"\"\n",
        "\n",
        "    # Define the column names you want to extract values from\n",
        "    columns_of_interest = ['heart_rate', 'hand_temperature', 'hand_3D_acceleration_16_x', 'hand_3D_acceleration_16_y',\n",
        "                           'hand_3D_acceleration_16_z', 'hand_3D_acceleration_6_x', 'hand_3D_acceleration_6_y',\n",
        "                           'hand_3D_acceleration_6_z', 'hand_3D_gyroscope_x', 'hand_3D_gyroscope_y',\n",
        "                           'hand_3D_gyroscope_z', 'hand_3D_magnetometer_x', 'hand_3D_magnetometer_y',\n",
        "                           'hand_3D_magnetometer_z', 'chest_temperature', 'chest_3D_acceleration_16_x',\n",
        "                           'chest_3D_acceleration_16_y', 'chest_3D_acceleration_16_z', 'chest_3D_acceleration_6_x',\n",
        "                           'chest_3D_acceleration_6_y', 'chest_3D_acceleration_6_z', 'chest_3D_gyroscope_x',\n",
        "                           'chest_3D_gyroscope_y', 'chest_3D_gyroscope_z', 'chest_3D_magnetometer_x',\n",
        "                           'chest_3D_magnetometer_y', 'chest_3D_magnetometer_z', 'ankle_temperature',\n",
        "                           'ankle_3D_acceleration_16_x', 'ankle_3D_acceleration_16_y', 'ankle_3D_acceleration_16_z',\n",
        "                           'ankle_3D_acceleration_6_x', 'ankle_3D_acceleration_6_y', 'ankle_3D_acceleration_6_z',\n",
        "                           'ankle_3D_gyroscope_x', 'ankle_3D_gyroscope_y', 'ankle_3D_gyroscope_z',\n",
        "                           'ankle_3D_magnetometer_x', 'ankle_3D_magnetometer_y', 'ankle_3D_magnetometer_z']\n",
        "\n",
        "    # Iterate over the columns of interest\n",
        "    for column in columns_of_interest:\n",
        "        # Get the value for the current column\n",
        "        value = df2[column].values[0]  # Directly access the single value in the column\n",
        "\n",
        "        # Add column name and value to the sentence\n",
        "        sentence += \"{}{}\".format(column, value)\n",
        "\n",
        "        # Add a space if it's not the last column\n",
        "        if column != columns_of_interest[-1]:\n",
        "            sentence += \" \"\n",
        "\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnHq9F5eqQ4y"
      },
      "outputs": [],
      "source": [
        "def sequencesToSentences(activitySequences):\n",
        "\tsentences = []\n",
        "\tlabel_sentences = []\n",
        "\n",
        "\tfor i in range(len(activitySequences)):\n",
        "\n",
        "\t\tsentence = generate_sentence(activitySequences[i])\n",
        "\n",
        "\t\tsentences.append(sentence)\n",
        "\t\tlabel_sentences.append(activitySequences[i].activity_id.values[0])\n",
        "\n",
        "\treturn sentences, label_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_yTS-okqP9z"
      },
      "outputs": [],
      "source": [
        "sentences, label_sentences = sequencesToSentences(activitySequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJC6OaJPtqqD"
      },
      "source": [
        "sentences indexization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-ICO1HoqP8K"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(filters='!\"#$%&()*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "indexed_sentences = tokenizer.texts_to_sequences(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcYvgcwwdsB6",
        "outputId": "4330e83c-6ccb-42c1-be58-59b20d377539"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4038"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tokenizer.word_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJVWe6qoW-Pb"
      },
      "outputs": [],
      "source": [
        "indexed_sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGticVzWt8z7"
      },
      "source": [
        "sliding windows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ekQTNSluIhI"
      },
      "outputs": [],
      "source": [
        "def slidingWindow(sequence,winSize,step=1):\n",
        "\n",
        "    try: it = iter(sequence)\n",
        "    except TypeError:\n",
        "        raise Exception(\"**ERROR** sequence must be iterable.\")\n",
        "    if not ((type(winSize) == type(0)) and (type(step) == type(0))):\n",
        "        raise Exception(\"**ERROR** type(winSize) and type(step) must be int.\")\n",
        "    if step > winSize:\n",
        "        raise Exception(\"**ERROR** step must not be larger than winSize.\")\n",
        "\n",
        "    numOfChunks = int(((len(sequence)-winSize)/step)+1)\n",
        "\n",
        "    # Do the work\n",
        "    if winSize > len(sequence):\n",
        "        yield sequence[0:len(sequence)]\n",
        "    else:\n",
        "        for i in range(0,numOfChunks*step,step):\n",
        "            yield sequence[i:i+winSize]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xd3Tdc3BqP6l"
      },
      "outputs": [],
      "source": [
        "X_windowed = []\n",
        "Y_windowed = []\n",
        "winSize = 50\n",
        "step = 1\n",
        "\n",
        "for i,s in enumerate(indexed_sentences):\n",
        "\tchunks = slidingWindow(s,winSize,step)\n",
        "\tfor chunk in chunks:\n",
        "\t\tX_windowed.append(chunk)\n",
        "\t\tY_windowed.append(label_sentences[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGox9uZbqP40"
      },
      "outputs": [],
      "source": [
        "padded_windows = pad_sequences(X_windowed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRJ6w_n9qP2r",
        "outputId": "a21d4e80-e811-4fb9-800b-3174ade4aec7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "STEP 8: save sliding windows and labels\n"
          ]
        }
      ],
      "source": [
        "Y_windowed = np.array(Y_windowed)\n",
        "\n",
        "## Save files ##\n",
        "print(\"STEP 8: save sliding windows and labels\")\n",
        "np.save(\"{}_{}_padded_x.npy\".format(\"lstm\",winSize), padded_windows)\n",
        "np.save(\"{}_{}_padded_y.npy\".format(\"lstm\",winSize), Y_windowed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A15ni7ctzeHh",
        "outputId": "369d4052-25b3-4cb5-f24c-3e3a6e238b14"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(14593, 50)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "padded_windows.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WazwE_R1zeF5",
        "outputId": "eb819e40-752e-40b0-917d-b98f71649278"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(50,)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "padded_windows[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtsgK1L4zeER"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSVdjNA9zeCh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bevUZucfclcN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWiTimhNzepZ"
      },
      "source": [
        "### main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K170Me41SWH1"
      },
      "outputs": [],
      "source": [
        "train_padded_windows = np.load(\"{}_{}_tpadded_x.npy\".format(\"lstm\", winSize))\n",
        "Y_train_windowed = np.load(\"{}_{}_tpadded_y.npy\".format(\"lstm\", winSize))\n",
        "test_padded_windows = np.load(\"{}_{}_ttpadded_x.npy\".format(\"lstm\", winSize))\n",
        "Y_test_windowed = np.load(\"{}_{}_ttpadded_y.npy\".format(\"lstm\", winSize))\n",
        "val_padded_windows = np.load(\"{}_{}_vpadded_x.npy\".format(\"lstm\", winSize))\n",
        "Y_val_windowed = np.load(\"{}_{}_vpadded_y.npy\".format(\"lstm\", winSize))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQOB79a4M--5",
        "outputId": "1c0d21a5-d1f9-4613-b730-1b3e1e135086"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4099,)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Y_train_windowed.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VyaI3IPEOC7"
      },
      "outputs": [],
      "source": [
        "x_train = train_padded_windows\n",
        "y_train = Y_train_windowed\n",
        "x_test = test_padded_windows\n",
        "y_test = Y_test_windowed\n",
        "x_val = val_padded_windows\n",
        "y_val = Y_val_windowed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MPw5g0WU2Py"
      },
      "source": [
        "#### FCN embedded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZh4QONrWUz8"
      },
      "outputs": [],
      "source": [
        "pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOm-LNerTsal"
      },
      "outputs": [],
      "source": [
        "class LSTMModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm1 = tf.keras.layers.LSTM(hidden_dim, return_sequences=True)\n",
        "        self.dropout = tf.keras.layers.Dropout(0.5)  # Example dropout rate\n",
        "        self.lstm2 = tf.keras.layers.LSTM(hidden_dim)\n",
        "        self.fc = tf.keras.layers.Dense(output_dim, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        embedded = self.embedding(inputs)\n",
        "        lstm_out1 = self.lstm1(embedded)\n",
        "        lstm_out2 = self.lstm2(self.dropout(lstm_out1))\n",
        "        output = self.fc(lstm_out2)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNhWLAOLPd4V"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 100\n",
        "hidden_dim = 128\n",
        "output_dim = 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0loC6vSapfb"
      },
      "outputs": [],
      "source": [
        "model = LSTMModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=3, batch_size=20, validation_data=(x_val, y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDlOLY-ZXrTi"
      },
      "source": [
        "#### predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkYl_HJYqPwv",
        "outputId": "9e2d88dd-b500-454d-b5e3-3b44d21a5f5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "56/56 [==============================] - 4s 52ms/step\n"
          ]
        }
      ],
      "source": [
        "predictions = model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9WB0frkqPuN",
        "outputId": "01a86ddf-4aa6-4336-c099-132269079bb6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1807, 25)"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51Go2kTiqPsU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class_predictions = np.argmax(predictions, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0ybWx0liL2z",
        "outputId": "9aed1514-9bf7-4184-bfe6-4e47a8969afb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.055623471882640586\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(y_test, class_predictions)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "zdX3CCKeMz2u",
        "4bySOOtMNf_Z",
        "GGJRmW8WM2F3",
        "2wzD1GFmmxbb",
        "anuL_g-JpzYp",
        "O-MCQ3kEp14o",
        "hWiTimhNzepZ",
        "9MPw5g0WU2Py",
        "CDlOLY-ZXrTi"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
