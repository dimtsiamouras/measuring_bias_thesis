{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOfWh9IPgHBv"
      },
      "source": [
        "https://github.com/dbouchabou/Fully-Convolutional-Network-Smart-Homes/tree/master"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eZvoGlUiADu",
        "outputId": "ae4a09be-1783-4b3f-d0eb-ea3f613d8987"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive' , force_remount = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "65A1BVG_f5_A"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVn1SGQPc1xP"
      },
      "source": [
        "### MHEALTH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvWCNaRec2De",
        "outputId": "b76959e7-dcba-416f-ee85-2bcbb0e04c78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/MHEALTHDATASET\n"
          ]
        }
      ],
      "source": [
        "cd \"/content/drive/MyDrive/MHEALTHDATASET/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "muNjBtYfc2h0"
      },
      "outputs": [],
      "source": [
        "df_mhealth = pd.read_pickle('mhealth_for_lstm.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "q7w1VUFadtVy"
      },
      "outputs": [],
      "source": [
        "data = df_mhealth\n",
        "grouped_data = df_mhealth.groupby('subject')\n",
        "train_data = pd.DataFrame()\n",
        "val_data = pd.DataFrame()\n",
        "test_data = pd.DataFrame()\n",
        "\n",
        "for _, group in grouped_data:\n",
        "    sorted_group = group.sort_index()\n",
        "\n",
        "    total_samples = len(sorted_group)\n",
        "    train_size = int(0.6 * total_samples)\n",
        "    val_size = int(0.2 * total_samples)\n",
        "    test_size = int(0.2 * total_samples)\n",
        "\n",
        "    train_group = sorted_group.iloc[val_size:val_size+train_size]\n",
        "    val_group = sorted_group.iloc[:val_size]\n",
        "    test_group = sorted_group.iloc[val_size + train_size:]\n",
        "\n",
        "    train_data = pd.concat([train_data, train_group])\n",
        "    test_data = pd.concat([test_data, test_group])\n",
        "    val_data = pd.concat([val_data, val_group])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = df_mhealth\n",
        "grouped_data = df_mhealth.groupby('subject')\n",
        "train_data = pd.DataFrame()\n",
        "val_data = pd.DataFrame()\n",
        "test_data = pd.DataFrame()\n",
        "\n",
        "for _, group in grouped_data:\n",
        "    sorted_group = group.sort_index()\n",
        "\n",
        "    total_samples = len(sorted_group)\n",
        "    train_size = int(0.6 * total_samples)\n",
        "    val_size = int(0.2 * total_samples)\n",
        "    test_size = int(0.2 * total_samples)\n",
        "\n",
        "    train_group = sorted_group.iloc[:train_size]\n",
        "    val_group = sorted_group.iloc[train_size:train_size+val_size]\n",
        "    test_group = sorted_group.iloc[val_size + train_size:]\n",
        "\n",
        "    train_data = pd.concat([train_data, train_group])\n",
        "    test_data = pd.concat([test_data, test_group])\n",
        "    val_data = pd.concat([val_data, val_group])"
      ],
      "metadata": {
        "id": "ocOmrIMJJowS"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "bR2fQ38ufjtR"
      },
      "outputs": [],
      "source": [
        "#scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "numerical_columns = ['acc_ch_x','acc_ch_y','acc_ch_z','ecg_sig_1', 'ecg_sig_2','acc_la_x','acc_la_y','acc_la_z','gyr_la_x','gyr_la_y','gyr_la_z','mag_la_x','mag_la_y','mag_la_z','acc_rw_x','acc_rw_y','acc_rw_z','gyr_rw_x','gyr_rw_y','gyr_rw_z','mag_rw_x','mag_rw_y','mag_rw_z',]\n",
        "scaler = StandardScaler()\n",
        "\n",
        "scaler.fit(train_data[numerical_columns])\n",
        "train_data[numerical_columns] = scaler.transform(train_data[numerical_columns])\n",
        "val_data[numerical_columns] = scaler.transform(val_data[numerical_columns])\n",
        "test_data[numerical_columns] = scaler.transform(test_data[numerical_columns])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "OrcY6WG5c2cO"
      },
      "outputs": [],
      "source": [
        "def segment_activities(df):\n",
        "    activitiesSeq = []\n",
        "\n",
        "    ponentialIndex = df.activity.ne(df.activity.shift())\n",
        "\n",
        "    ii = np.where(ponentialIndex == True)[0]\n",
        "\n",
        "    for i,end in enumerate(ii):\n",
        "        if i > 0 :\n",
        "\n",
        "          dftmp = df[ii[i-1]:end]\n",
        "          activitiesSeq.append(dftmp)\n",
        "    return activitiesSeq\n",
        "\n",
        "train_activitySequences = segment_activities(train_data)\n",
        "val_activitySequences = segment_activities(val_data)\n",
        "test_activitySequences = segment_activities(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "wPGIxUB-c2am"
      },
      "outputs": [],
      "source": [
        "def generate_sentence(df2):\n",
        "    sentence = \"\"\n",
        "\n",
        "    #columns for which to retain values\n",
        "    columns_of_interest = ['acc_ch_x', 'acc_ch_y', 'acc_ch_z', 'ecg_sig_1', 'ecg_sig_2',\n",
        "       'acc_la_x', 'acc_la_y', 'acc_la_z', 'gyr_la_x', 'gyr_la_y', 'gyr_la_z',\n",
        "       'mag_la_x', 'mag_la_y', 'mag_la_z', 'acc_rw_x', 'acc_rw_y', 'acc_rw_z',\n",
        "       'gyr_rw_x', 'gyr_rw_y', 'gyr_rw_z', 'mag_rw_x', 'mag_rw_y', 'mag_rw_z',\n",
        "       'activity', 'subject']\n",
        "\n",
        "    #iterate over columns\n",
        "    for column in columns_of_interest:\n",
        "        #column value\n",
        "        value = df2[column].values[0]  # Directly access the single value in the column\n",
        "\n",
        "        #column name and val\n",
        "        sentence += \"{}{}\".format(column, value)\n",
        "\n",
        "        #space if not last col\n",
        "        if column != columns_of_interest[-1]:\n",
        "            sentence += \" \"\n",
        "\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "0K-s-GlYc2ZL"
      },
      "outputs": [],
      "source": [
        "def sequencesToSentences(activitySequences):\n",
        "\tsentences = []\n",
        "\tlabel_sentences = []\n",
        "\n",
        "\tfor i in range(len(activitySequences)):\n",
        "\n",
        "\t\tsentence = generate_sentence(activitySequences[i])\n",
        "\n",
        "\t\tsentences.append(sentence)\n",
        "\t\tlabel_sentences.append(activitySequences[i].activity.values[0])\n",
        "\n",
        "\treturn sentences, label_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "OAmszOMMc2XU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "717dba83-22b4-400f-d731-c55bbc89361d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function WeakKeyDictionary.__init__.<locals>.remove at 0x7b06f8421a20>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/weakref.py\", line 370, in remove\n",
            "    def remove(k, selfref=ref(self)):\n",
            "KeyboardInterrupt: \n"
          ]
        }
      ],
      "source": [
        "train_sentences, train_label_sentences = sequencesToSentences(train_activitySequences)\n",
        "val_sentences, val_label_sentences = sequencesToSentences(val_activitySequences)\n",
        "test_sentences, test_label_sentences = sequencesToSentences(test_activitySequences)\n",
        "\n",
        "tokenizer = Tokenizer(filters='!\"#$%&()*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "combined_sentences = train_sentences + val_sentences + test_sentences\n",
        "tokenizer.fit_on_texts(combined_sentences)\n",
        "\n",
        "# Tokenize train, validation, and test sets\n",
        "train_indexed_sentences = tokenizer.texts_to_sequences(train_sentences)\n",
        "val_indexed_sentences = tokenizer.texts_to_sequences(val_sentences)\n",
        "test_indexed_sentences = tokenizer.texts_to_sequences(test_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "ANNSYwKIc2Vc"
      },
      "outputs": [],
      "source": [
        "def slidingWindow(sequence,winSize,step=1):\n",
        "\n",
        "    try: it = iter(sequence)\n",
        "    except TypeError:\n",
        "        raise Exception(\"**ERROR** sequence must be iterable.\")\n",
        "    if not ((type(winSize) == type(0)) and (type(step) == type(0))):\n",
        "        raise Exception(\"**ERROR** type(winSize) and type(step) must be int.\")\n",
        "    if step > winSize:\n",
        "        raise Exception(\"**ERROR** step must not be larger than winSize.\")\n",
        "\n",
        "    numOfChunks = int(((len(sequence)-winSize)/step)+1)\n",
        "\n",
        "    if winSize > len(sequence):\n",
        "        yield sequence[0:len(sequence)]\n",
        "    else:\n",
        "        for i in range(0,numOfChunks*step,step):\n",
        "            yield sequence[i:i+winSize]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "Ghai8RKmc2Tp"
      },
      "outputs": [],
      "source": [
        "X_train_windowed = []\n",
        "Y_train_windowed = []\n",
        "winSize = 50\n",
        "step = 1\n",
        "\n",
        "for i,s in enumerate(train_indexed_sentences):\n",
        "\tchunks = slidingWindow(s,winSize,step)\n",
        "\tfor chunk in chunks:\n",
        "\t\tX_train_windowed.append(chunk)\n",
        "\t\tY_train_windowed.append(train_label_sentences[i])\n",
        "\n",
        "X_val_windowed = []\n",
        "Y_val_windowed = []\n",
        "winSize = 50\n",
        "step = 1\n",
        "\n",
        "for i,s in enumerate(val_indexed_sentences):\n",
        "\tchunks = slidingWindow(s,winSize,step)\n",
        "\tfor chunk in chunks:\n",
        "\t\tX_val_windowed.append(chunk)\n",
        "\t\tY_val_windowed.append(val_label_sentences[i])\n",
        "\n",
        "\n",
        "X_test_windowed = []\n",
        "Y_test_windowed = []\n",
        "winSize = 50\n",
        "step = 1\n",
        "\n",
        "for i,s in enumerate(test_indexed_sentences):\n",
        "\tchunks = slidingWindow(s,winSize,step)\n",
        "\tfor chunk in chunks:\n",
        "\t\tX_test_windowed.append(chunk)\n",
        "\t\tY_test_windowed.append(test_label_sentences[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "4xFxxCbJfAt5"
      },
      "outputs": [],
      "source": [
        "train_padded_windows = pad_sequences(X_train_windowed)\n",
        "test_padded_windows = pad_sequences(X_test_windowed)\n",
        "val_padded_windows = pad_sequences(X_val_windowed)\n",
        "\n",
        "Y_train_windowed = np.array(Y_train_windowed)\n",
        "Y_test_windowed = np.array(Y_test_windowed)\n",
        "Y_val_windowed = np.array(Y_val_windowed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = train_padded_windows\n",
        "y_train = Y_train_windowed\n",
        "x_test = test_padded_windows\n",
        "y_test = Y_test_windowed\n",
        "x_val = val_padded_windows\n",
        "y_val = Y_val_windowed"
      ],
      "metadata": {
        "id": "1fqUKGzXI9ns"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import entropy\n",
        "\n",
        "js_divergences = np.zeros(x_train.shape[1])\n",
        "for i in range(x_train.shape[1]):\n",
        "    # Compute histograms with the same bins for both X_train and X_test\n",
        "    bins = max(len(np.unique(x_train[:, i])), len(np.unique(x_test[:, i])))\n",
        "    p, _ = np.histogram(x_train[:, i], bins=bins, density=True)\n",
        "    q, _ = np.histogram(x_test[:, i], bins=bins, density=True)\n",
        "    m = 0.5 * (p + q)\n",
        "    js_divergences[i] = 0.5 * (entropy(p, m) + entropy(q, m))\n",
        "\n",
        "avg_js_divergence = np.mean(js_divergences)\n",
        "print(\"Average Jensen-Shannon divergence:\", avg_js_divergence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyDXfjwHI9lT",
        "outputId": "f5061b9c-6ee7-4605-8375-8502527841ad"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Jensen-Shannon divergence: 0.22045810936700894\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwxWpu9zauiN"
      },
      "source": [
        "### PAMAP2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s562sAmxkAZd",
        "outputId": "16c432b0-74bd-4c24-ebcc-f6244513103e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/PAMAP2_Dataset\n"
          ]
        }
      ],
      "source": [
        "cd \"/content/drive/MyDrive/PAMAP2_Dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fhYNvGgOiLrC"
      },
      "outputs": [],
      "source": [
        "data = pd.read_pickle('pamap2_for_lstm.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNsZeT_fmNQv"
      },
      "source": [
        "Take last"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bywwkTMpnSgn"
      },
      "source": [
        "#### ~ 200k samples per id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FrANPIlUtctQ"
      },
      "outputs": [],
      "source": [
        "columns_of_interest = ['time_stamp', 'activity_id','heart_rate', 'hand_temperature','hand_3D_acceleration_16_x', 'hand_3D_acceleration_16_y','hand_3D_acceleration_16_z', 'hand_3D_gyroscope_x', 'hand_3D_gyroscope_y', 'hand_3D_gyroscope_z','chest_3D_acceleration_16_x', 'chest_3D_acceleration_16_y','chest_3D_acceleration_16_z','chest_3D_gyroscope_x', 'chest_3D_gyroscope_y', 'chest_3D_gyroscope_z','ankle_3D_acceleration_16_x', 'ankle_3D_acceleration_16_y','ankle_3D_acceleration_16_z','ankle_3D_gyroscope_x', 'ankle_3D_gyroscope_y', 'ankle_3D_gyroscope_z', 'id']\n",
        "\n",
        "data = data[columns_of_interest]\n",
        "data = data[data['id'] != 109]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "XkYoIK6huITF"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "grouped_data = data.groupby('id')\n",
        "train_data = pd.DataFrame()\n",
        "test_data = pd.DataFrame()\n",
        "val_data = pd.DataFrame()\n",
        "\n",
        "for _, group in grouped_data:\n",
        "    sorted_group = group.sort_values(by='time_stamp', ascending=True)\n",
        "\n",
        "    # Calculate the size of training data dynamically based on the first 60% of rows\n",
        "    total_samples = len(sorted_group)\n",
        "    train_size = int(0.6 * total_samples)\n",
        "    val_size = int(0.2 * total_samples)\n",
        "    test_size = int(0.2 * total_samples)\n",
        "\n",
        "    train_group = sorted_group.iloc[:train_size]\n",
        "    val_group = sorted_group.iloc[train_size:val_size]\n",
        "    test_group = sorted_group.iloc[val_size + train_size:]\n",
        "\n",
        "    train_data = pd.concat([train_data, train_group])\n",
        "    val_data = pd.concat([val_data, val_group])\n",
        "    test_data = pd.concat([test_data, test_group])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdlT_IOwnere"
      },
      "source": [
        "scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "L0WnheNNnekA"
      },
      "outputs": [],
      "source": [
        "train_activity_id = train_data['activity_id']\n",
        "train_sub_id = train_data['id']\n",
        "test_activity_id = test_data['activity_id']\n",
        "test_sub_id = test_data['id']\n",
        "val_activity_id = val_data['activity_id']\n",
        "val_sub_id = val_data['id']\n",
        "\n",
        "train_df_without_id = train_data.drop(columns=['activity_id', 'id'])\n",
        "test_df_without_id = test_data.drop(columns=['activity_id', 'id'])\n",
        "val_df_without_id = val_data.drop(columns=['activity_id', 'id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "DnfDed-9nmu3"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "numerical_columns = train_df_without_id.select_dtypes(include=['number']).columns\n",
        "\n",
        "scaler = StandardScaler()\n",
        "train_scaled = scaler.fit_transform(train_df_without_id[numerical_columns])\n",
        "train_scaled_df = pd.DataFrame(train_scaled, columns=numerical_columns, index=train_df_without_id.index)\n",
        "\n",
        "test_scaled = scaler.transform(test_df_without_id[numerical_columns])\n",
        "test_scaled_df = pd.DataFrame(test_scaled, columns=numerical_columns, index=test_df_without_id.index)\n",
        "\n",
        "val_scaled = scaler.transform(val_df_without_id[numerical_columns])\n",
        "val_scaled_df = pd.DataFrame(val_scaled, columns=numerical_columns, index=val_df_without_id.index)\n",
        "\n",
        "train_scaled_df = pd.concat([train_activity_id, train_sub_id, train_scaled_df], axis=1)\n",
        "test_scaled_df = pd.concat([test_activity_id, test_sub_id, test_scaled_df], axis=1)\n",
        "val_scaled_df = pd.concat([val_activity_id, val_sub_id, val_scaled_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "9-LCTycdiLkp"
      },
      "outputs": [],
      "source": [
        "def segment_activities(df):\n",
        "    activitiesSeq = []\n",
        "\n",
        "    ponentialIndex = df.activity_id.ne(df.activity_id.shift())\n",
        "\n",
        "    ii = np.where(ponentialIndex == True)[0]\n",
        "\n",
        "    for i,end in enumerate(ii):\n",
        "        if i > 0 :\n",
        "\n",
        "          dftmp = df[ii[i-1]:end]\n",
        "          activitiesSeq.append(dftmp)\n",
        "    return activitiesSeq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "BB2VudGYqEzt"
      },
      "outputs": [],
      "source": [
        "train_activitySequences = segment_activities(train_data)\n",
        "val_activitySequences = segment_activities(val_data)\n",
        "test_activitySequences = segment_activities(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMtD9IEiAlmG"
      },
      "source": [
        "#### with my columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "_SzDwoJSAkCb"
      },
      "outputs": [],
      "source": [
        "def generate_sentence(df2):\n",
        "    sentence = \"\"\n",
        "\n",
        "    # Define the column names you want to extract values from\n",
        "    columns_of_interest = ['time_stamp', 'activity_id','heart_rate', 'hand_temperature','hand_3D_acceleration_16_x', 'hand_3D_acceleration_16_y','hand_3D_acceleration_16_z', 'hand_3D_gyroscope_x', 'hand_3D_gyroscope_y', 'hand_3D_gyroscope_z','chest_3D_acceleration_16_x', 'chest_3D_acceleration_16_y','chest_3D_acceleration_16_z','chest_3D_gyroscope_x', 'chest_3D_gyroscope_y', 'chest_3D_gyroscope_z','ankle_3D_acceleration_16_x', 'ankle_3D_acceleration_16_y','ankle_3D_acceleration_16_z','ankle_3D_gyroscope_x', 'ankle_3D_gyroscope_y', 'ankle_3D_gyroscope_z', 'id']\n",
        "\n",
        "    # Iterate over the columns of interest\n",
        "    for column in columns_of_interest:\n",
        "        # Get the value for the current column\n",
        "        value = df2[column].values[0]  # Directly access the single value in the column\n",
        "\n",
        "        # Add column name and value to the sentence\n",
        "        sentence += \"{}{}\".format(column, value)\n",
        "\n",
        "        # Add a space if it's not the last column\n",
        "        if column != columns_of_interest[-1]:\n",
        "            sentence += \" \"\n",
        "\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "tDZWx6V0Aj6k"
      },
      "outputs": [],
      "source": [
        "def sequencesToSentences(activitySequences):\n",
        "\tsentences = []\n",
        "\tlabel_sentences = []\n",
        "\n",
        "\tfor i in range(len(activitySequences)):\n",
        "\n",
        "\t\tsentence = generate_sentence(activitySequences[i])\n",
        "\n",
        "\t\tsentences.append(sentence)\n",
        "\t\tlabel_sentences.append(activitySequences[i].activity_id.values[0])\n",
        "\n",
        "\treturn sentences, label_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "SaInQMtIAo_a"
      },
      "outputs": [],
      "source": [
        "train_sentences, train_label_sentences = sequencesToSentences(train_activitySequences)\n",
        "val_sentences, val_label_sentences = sequencesToSentences(val_activitySequences)\n",
        "test_sentences, test_label_sentences = sequencesToSentences(test_activitySequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hu-K-UgPuFri"
      },
      "source": [
        "#### use 1 tokenizer for all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "QcKPFSnWuENi"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(filters='!\"#$%&()*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "combined_sentences = train_sentences + val_sentences + test_sentences\n",
        "tokenizer.fit_on_texts(combined_sentences)\n",
        "\n",
        "# Tokenize train, validation, and test sets\n",
        "train_indexed_sentences = tokenizer.texts_to_sequences(train_sentences)\n",
        "val_indexed_sentences = tokenizer.texts_to_sequences(val_sentences)\n",
        "test_indexed_sentences = tokenizer.texts_to_sequences(test_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "tqWPQ1VEB0Hh"
      },
      "outputs": [],
      "source": [
        "def slidingWindow(sequence,winSize,step=1):\n",
        "\n",
        "    try: it = iter(sequence)\n",
        "    except TypeError:\n",
        "        raise Exception(\"**ERROR** sequence must be iterable.\")\n",
        "    if not ((type(winSize) == type(0)) and (type(step) == type(0))):\n",
        "        raise Exception(\"**ERROR** type(winSize) and type(step) must be int.\")\n",
        "    if step > winSize:\n",
        "        raise Exception(\"**ERROR** step must not be larger than winSize.\")\n",
        "\n",
        "    numOfChunks = int(((len(sequence)-winSize)/step)+1)\n",
        "\n",
        "    # Do the work\n",
        "    if winSize > len(sequence):\n",
        "        yield sequence[0:len(sequence)]\n",
        "    else:\n",
        "        for i in range(0,numOfChunks*step,step):\n",
        "            yield sequence[i:i+winSize]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "ZD75iENMB0-V"
      },
      "outputs": [],
      "source": [
        "winSize = 50\n",
        "step = 1\n",
        "\n",
        "X_train_windowed = []\n",
        "Y_train_windowed = []\n",
        "for i,s in enumerate(train_indexed_sentences):\n",
        "\tchunks = slidingWindow(s,winSize,step)\n",
        "\tfor chunk in chunks:\n",
        "\t\tX_train_windowed.append(chunk)\n",
        "\t\tY_train_windowed.append(train_label_sentences[i])\n",
        "\n",
        "X_val_windowed = []\n",
        "Y_val_windowed = []\n",
        "for i,s in enumerate(val_indexed_sentences):\n",
        "\tchunks = slidingWindow(s,winSize,step)\n",
        "\tfor chunk in chunks:\n",
        "\t\tX_val_windowed.append(chunk)\n",
        "\t\tY_val_windowed.append(val_label_sentences[i])\n",
        "\n",
        "\n",
        "X_test_windowed = []\n",
        "Y_test_windowed = []\n",
        "for i,s in enumerate(test_indexed_sentences):\n",
        "\tchunks = slidingWindow(s,winSize,step)\n",
        "\tfor chunk in chunks:\n",
        "\t\tX_test_windowed.append(chunk)\n",
        "\t\tY_test_windowed.append(test_label_sentences[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "mCeoXDmhChpH"
      },
      "outputs": [],
      "source": [
        "train_padded_windows = pad_sequences(X_train_windowed)\n",
        "test_padded_windows = pad_sequences(X_test_windowed)\n",
        "val_padded_windows = pad_sequences(X_val_windowed)\n",
        "\n",
        "Y_train_windowed = np.array(Y_train_windowed)\n",
        "Y_test_windowed = np.array(Y_test_windowed)\n",
        "Y_val_windowed = np.array(Y_val_windowed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = train_padded_windows\n",
        "y_train = Y_train_windowed\n",
        "x_test = test_padded_windows\n",
        "y_test = Y_test_windowed\n",
        "x_val = val_padded_windows\n",
        "y_val = Y_val_windowed"
      ],
      "metadata": {
        "id": "Jh-I-jION6sW"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import entropy\n",
        "\n",
        "js_divergences = np.zeros(x_train.shape[1])\n",
        "for i in range(x_train.shape[1]):\n",
        "    # Compute histograms with the same bins for both X_train and X_test\n",
        "    bins = max(len(np.unique(x_train[:, i])), len(np.unique(x_test[:, i])))\n",
        "    p, _ = np.histogram(x_train[:, i], bins=bins, density=True)\n",
        "    q, _ = np.histogram(x_test[:, i], bins=bins, density=True)\n",
        "    m = 0.5 * (p + q)\n",
        "    js_divergences[i] = 0.5 * (entropy(p, m) + entropy(q, m))\n",
        "\n",
        "avg_js_divergence = np.mean(js_divergences)\n",
        "print(\"Average Jensen-Shannon divergence:\", avg_js_divergence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1BITkhUL5gS",
        "outputId": "d19285eb-55d0-4a55-c2dd-286281412488"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Jensen-Shannon divergence: 0.12993332932141433\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWiTimhNzepZ"
      },
      "source": [
        "### main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MPw5g0WU2Py"
      },
      "source": [
        "#### FCN embedded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZh4QONrWUz8"
      },
      "outputs": [],
      "source": [
        "pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "1xX8s7veztjh"
      },
      "outputs": [],
      "source": [
        "class LSTMModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm1 = tf.keras.layers.LSTM(hidden_dim, return_sequences=True)\n",
        "        self.dropout = tf.keras.layers.Dropout(0.5)  # Example dropout rate\n",
        "        self.lstm2 = tf.keras.layers.LSTM(hidden_dim)\n",
        "        self.fc = tf.keras.layers.Dense(output_dim, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        embedded = self.embedding(inputs)\n",
        "        lstm_out1 = self.lstm1(embedded)\n",
        "        lstm_out2 = self.lstm2(self.dropout(lstm_out1))\n",
        "        output = self.fc(lstm_out2)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "aNhWLAOLPd4V"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 100\n",
        "hidden_dim = 128\n",
        "output_dim = 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "k0loC6vSapfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3ae3164-4e1a-4e9f-f194-c24875a781ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "265/265 [==============================] - 42s 135ms/step - loss: 2.4133 - accuracy: 0.1816 - val_loss: 2.6937 - val_accuracy: 0.1790\n",
            "Epoch 2/3\n",
            "265/265 [==============================] - 36s 134ms/step - loss: 1.4233 - accuracy: 0.4688 - val_loss: 1.5773 - val_accuracy: 0.4356\n",
            "Epoch 3/3\n",
            "265/265 [==============================] - 43s 163ms/step - loss: 0.7029 - accuracy: 0.7485 - val_loss: 1.2875 - val_accuracy: 0.6155\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b06cdebd510>"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "model = LSTMModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=3, batch_size=20, validation_data=(x_val, y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDlOLY-ZXrTi"
      },
      "source": [
        "#### predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "JkYl_HJYqPwv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef228a98-6f16-4c4a-afb3-b76993f8c05a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39/39 [==============================] - 3s 46ms/step\n"
          ]
        }
      ],
      "source": [
        "predictions = model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "51Go2kTiqPsU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class_predictions = np.argmax(predictions, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "w0ybWx0liL2z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa528cda-5ad5-4481-fd6a-8857b9d1228c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.40468497576736673\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(y_test, class_predictions)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "YVn1SGQPc1xP",
        "lwxWpu9zauiN",
        "hWiTimhNzepZ",
        "9MPw5g0WU2Py",
        "CDlOLY-ZXrTi"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
