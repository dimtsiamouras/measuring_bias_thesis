{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOfWh9IPgHBv"
      },
      "source": [
        "https://github.com/dbouchabou/Fully-Convolutional-Network-Smart-Homes/tree/master"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eZvoGlUiADu",
        "outputId": "1d6fea95-fc4b-431a-c984-8b92f8f8fa87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive' , force_remount = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65A1BVG_f5_A"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "pip install tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MHEALTH"
      ],
      "metadata": {
        "id": "TMpU0rx5IVWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd \"/content/drive/MyDrive/MHEALTHDATASET/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umcqJiLvIVt8",
        "outputId": "25279052-d742-4cb3-f6f5-5bcc61d9d7ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/MHEALTHDATASET\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_mhealth = pd.read_pickle('mhealth_for_lstm.pkl')"
      ],
      "metadata": {
        "id": "URBox8sRNZgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### sliding windows"
      ],
      "metadata": {
        "id": "_qGcIy7bIrDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def segment_activities(df):\n",
        "    activitiesSeq = []\n",
        "\n",
        "    ponentialIndex = df.activity.ne(df.activity.shift())\n",
        "\n",
        "    ii = np.where(ponentialIndex == True)[0]\n",
        "\n",
        "    for i,end in enumerate(ii):\n",
        "        if i > 0 :\n",
        "\n",
        "          dftmp = df[ii[i-1]:end]\n",
        "          activitiesSeq.append(dftmp)\n",
        "    return activitiesSeq"
      ],
      "metadata": {
        "id": "Mj-vbY9dIWir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "activitySequences = segment_activities(df_mhealth)"
      ],
      "metadata": {
        "id": "2j9-rtGoIWgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sentence(df2):\n",
        "    sentence = \"\"\n",
        "\n",
        "    #columns for which to retain values\n",
        "    columns_of_interest = ['acc_ch_x', 'acc_ch_y', 'acc_ch_z', 'ecg_sig_1', 'ecg_sig_2',\n",
        "       'acc_la_x', 'acc_la_y', 'acc_la_z', 'gyr_la_x', 'gyr_la_y', 'gyr_la_z',\n",
        "       'mag_la_x', 'mag_la_y', 'mag_la_z', 'acc_rw_x', 'acc_rw_y', 'acc_rw_z',\n",
        "       'gyr_rw_x', 'gyr_rw_y', 'gyr_rw_z', 'mag_rw_x', 'mag_rw_y', 'mag_rw_z',\n",
        "       'activity', 'subject']\n",
        "\n",
        "    #iterate over columns\n",
        "    for column in columns_of_interest:\n",
        "        #get col value\n",
        "        value = df2[column].values[0]\n",
        "\n",
        "        #add col name and value\n",
        "        sentence += \"{}{}\".format(column, value)\n",
        "\n",
        "        #space for all columns except last\n",
        "        if column != columns_of_interest[-1]:\n",
        "            sentence += \" \"\n",
        "\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "DeQU6428IWdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sequencesToSentences(activitySequences):\n",
        "\tsentences = []\n",
        "\tlabel_sentences = []\n",
        "\n",
        "\tfor i in range(len(activitySequences)):\n",
        "\n",
        "\t\tsentence = generate_sentence(activitySequences[i])\n",
        "\n",
        "\t\tsentences.append(sentence)\n",
        "\t\tlabel_sentences.append(activitySequences[i].activity.values[0])\n",
        "\n",
        "\treturn sentences, label_sentences"
      ],
      "metadata": {
        "id": "32lhdP2DJDpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences, label_sentences = sequencesToSentences(activitySequences)"
      ],
      "metadata": {
        "id": "DjYymB5hIWZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tokenizer and indexing"
      ],
      "metadata": {
        "id": "yzclJdijKKm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(filters='!\"#$%&()*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "indexed_sentences = tokenizer.texts_to_sequences(sentences)"
      ],
      "metadata": {
        "id": "aWxSOxomIWX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### sliding window data segmentation"
      ],
      "metadata": {
        "id": "q6Zd4NqsK2Cr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def slidingWindow(sequence,winSize,step=1):\n",
        "\n",
        "    try: it = iter(sequence)\n",
        "    except TypeError:\n",
        "        raise Exception(\"**ERROR** sequence must be iterable.\")\n",
        "    if not ((type(winSize) == type(0)) and (type(step) == type(0))):\n",
        "        raise Exception(\"**ERROR** type(winSize) and type(step) must be int.\")\n",
        "    if step > winSize:\n",
        "        raise Exception(\"**ERROR** step must not be larger than winSize.\")\n",
        "\n",
        "    numOfChunks = int(((len(sequence)-winSize)/step)+1)\n",
        "\n",
        "    # Do the work\n",
        "    if winSize > len(sequence):\n",
        "        yield sequence[0:len(sequence)]\n",
        "    else:\n",
        "        for i in range(0,numOfChunks*step,step):\n",
        "            yield sequence[i:i+winSize]"
      ],
      "metadata": {
        "id": "Nt5ESVZnIWRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_windowed = []\n",
        "Y_windowed = []\n",
        "winSize = 50\n",
        "step = 1\n",
        "\n",
        "for i,s in enumerate(indexed_sentences):\n",
        "\tchunks = slidingWindow(s,winSize,step)\n",
        "\tfor chunk in chunks:\n",
        "\t\tX_windowed.append(chunk)\n",
        "\t\tY_windowed.append(label_sentences[i])"
      ],
      "metadata": {
        "id": "QRpCQrtSIWPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_x = pad_sequences(X_windowed)\n",
        "padded_y = np.array(Y_windowed)"
      ],
      "metadata": {
        "id": "LPGGGM4ZLAgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import entropy\n",
        "\n",
        "js_divergences = np.zeros(x_train.shape[1])\n",
        "for i in range(x_train.shape[1]):\n",
        "    # Compute histograms with the same bins for both X_train and X_test\n",
        "    bins = max(len(np.unique(x_train[:, i])), len(np.unique(x_test[:, i])))\n",
        "    p, _ = np.histogram(x_train[:, i], bins=bins, density=True)\n",
        "    q, _ = np.histogram(x_test[:, i], bins=bins, density=True)\n",
        "    m = 0.5 * (p + q)\n",
        "    js_divergences[i] = 0.5 * (entropy(p, m) + entropy(q, m))\n",
        "\n",
        "avg_js_divergence = np.mean(js_divergences)\n",
        "print(\"Average Jensen-Shannon divergence:\", avg_js_divergence)"
      ],
      "metadata": {
        "id": "p3E03jJOLAbw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33a24d82-0be6-45f6-c895-186ad2642f36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Jensen-Shannon divergence: 0.12609317940121165\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PAMAP2"
      ],
      "metadata": {
        "id": "pz0McM3_ILOO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s562sAmxkAZd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a62f7459-dec3-4313-ec28-c4dcc0c58fb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/PAMAP2_Dataset\n"
          ]
        }
      ],
      "source": [
        "cd \"/content/drive/MyDrive/PAMAP2_Dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhYNvGgOiLrC"
      },
      "outputs": [],
      "source": [
        "data = pd.read_pickle('pamap2_for_lstm.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "scaler"
      ],
      "metadata": {
        "id": "fD-Qnu73zzeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "activity_id = data['activity_id']\n",
        "sub_id = data['id']\n",
        "\n",
        "df_without_activity_id = data.drop(columns=['activity_id', 'id'])\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(df_without_activity_id)\n",
        "data = pd.DataFrame(data_scaled, columns=df_without_activity_id.columns, index=df_without_activity_id.index)\n",
        "data = pd.concat([activity_id, sub_id, data], axis=1)"
      ],
      "metadata": {
        "id": "9ggiK9TGc4ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-LCTycdiLkp"
      },
      "outputs": [],
      "source": [
        "def segment_activities(df):\n",
        "    activitiesSeq = []\n",
        "\n",
        "    ponentialIndex = df.activity_id.ne(df.activity_id.shift())\n",
        "\n",
        "    ii = np.where(ponentialIndex == True)[0]\n",
        "\n",
        "    for i,end in enumerate(ii):\n",
        "        if i > 0 :\n",
        "\n",
        "          dftmp = df[ii[i-1]:end]\n",
        "          activitiesSeq.append(dftmp)\n",
        "    return activitiesSeq\n",
        "\n",
        "activitySequences = segment_activities(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-MCQ3kEp14o"
      },
      "source": [
        "#### with my columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YB0mTFO5p1od"
      },
      "outputs": [],
      "source": [
        "def generate_sentence(df2):\n",
        "    sentence = \"\"\n",
        "\n",
        "    #columns for which to retain values\n",
        "    columns_of_interest = ['heart_rate', 'hand_temperature', 'hand_3D_acceleration_16_x', 'hand_3D_acceleration_16_y',\n",
        "                           'hand_3D_acceleration_16_z', 'hand_3D_acceleration_6_x', 'hand_3D_acceleration_6_y',\n",
        "                           'hand_3D_acceleration_6_z', 'hand_3D_gyroscope_x', 'hand_3D_gyroscope_y',\n",
        "                           'hand_3D_gyroscope_z', 'hand_3D_magnetometer_x', 'hand_3D_magnetometer_y',\n",
        "                           'hand_3D_magnetometer_z', 'chest_temperature', 'chest_3D_acceleration_16_x',\n",
        "                           'chest_3D_acceleration_16_y', 'chest_3D_acceleration_16_z', 'chest_3D_acceleration_6_x',\n",
        "                           'chest_3D_acceleration_6_y', 'chest_3D_acceleration_6_z', 'chest_3D_gyroscope_x',\n",
        "                           'chest_3D_gyroscope_y', 'chest_3D_gyroscope_z', 'chest_3D_magnetometer_x',\n",
        "                           'chest_3D_magnetometer_y', 'chest_3D_magnetometer_z', 'ankle_temperature',\n",
        "                           'ankle_3D_acceleration_16_x', 'ankle_3D_acceleration_16_y', 'ankle_3D_acceleration_16_z',\n",
        "                           'ankle_3D_acceleration_6_x', 'ankle_3D_acceleration_6_y', 'ankle_3D_acceleration_6_z',\n",
        "                           'ankle_3D_gyroscope_x', 'ankle_3D_gyroscope_y', 'ankle_3D_gyroscope_z',\n",
        "                           'ankle_3D_magnetometer_x', 'ankle_3D_magnetometer_y', 'ankle_3D_magnetometer_z']\n",
        "\n",
        "    #iterate over columns\n",
        "    for column in columns_of_interest:\n",
        "        value = df2[column].values[0]  # Directly access the single value in the column\n",
        "\n",
        "        sentence += \"{}{}\".format(column, value)\n",
        "\n",
        "        if column != columns_of_interest[-1]:\n",
        "            sentence += \" \"\n",
        "\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnHq9F5eqQ4y"
      },
      "outputs": [],
      "source": [
        "def sequencesToSentences(activitySequences):\n",
        "\tsentences = []\n",
        "\tlabel_sentences = []\n",
        "\n",
        "\tfor i in range(len(activitySequences)):\n",
        "\n",
        "\t\tsentence = generate_sentence(activitySequences[i])\n",
        "\n",
        "\t\tsentences.append(sentence)\n",
        "\t\tlabel_sentences.append(activitySequences[i].activity_id.values[0])\n",
        "\n",
        "\treturn sentences, label_sentences\n",
        "\n",
        "sentences, label_sentences = sequencesToSentences(activitySequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJC6OaJPtqqD"
      },
      "source": [
        "sentences indexization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-ICO1HoqP8K"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(filters='!\"#$%&()*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "indexed_sentences = tokenizer.texts_to_sequences(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGticVzWt8z7"
      },
      "source": [
        "sliding windows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ekQTNSluIhI"
      },
      "outputs": [],
      "source": [
        "def slidingWindow(sequence,winSize,step=1):\n",
        "\n",
        "    try: it = iter(sequence)\n",
        "    except TypeError:\n",
        "        raise Exception(\"**ERROR** sequence must be iterable.\")\n",
        "    if not ((type(winSize) == type(0)) and (type(step) == type(0))):\n",
        "        raise Exception(\"**ERROR** type(winSize) and type(step) must be int.\")\n",
        "    if step > winSize:\n",
        "        raise Exception(\"**ERROR** step must not be larger than winSize.\")\n",
        "\n",
        "    numOfChunks = int(((len(sequence)-winSize)/step)+1)\n",
        "\n",
        "    # Do the work\n",
        "    if winSize > len(sequence):\n",
        "        yield sequence[0:len(sequence)]\n",
        "    else:\n",
        "        for i in range(0,numOfChunks*step,step):\n",
        "            yield sequence[i:i+winSize]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xd3Tdc3BqP6l"
      },
      "outputs": [],
      "source": [
        "X_windowed = []\n",
        "Y_windowed = []\n",
        "winSize = 50\n",
        "step = 1\n",
        "\n",
        "for i,s in enumerate(indexed_sentences):\n",
        "\tchunks = slidingWindow(s,winSize,step)\n",
        "\tfor chunk in chunks:\n",
        "\t\tX_windowed.append(chunk)\n",
        "\t\tY_windowed.append(label_sentences[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGox9uZbqP40"
      },
      "outputs": [],
      "source": [
        "padded_x = pad_sequences(X_windowed)\n",
        "padded_y = np.array(Y_windowed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(padded_x, padded_y, test_size=(1 - 0.6), random_state=42)\n",
        "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "aNBC-dOT1TRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import entropy\n",
        "\n",
        "js_divergences = np.zeros(x_train.shape[1])\n",
        "for i in range(x_train.shape[1]):\n",
        "    # Compute histograms with the same bins for both X_train and X_test\n",
        "    bins = max(len(np.unique(x_train[:, i])), len(np.unique(x_test[:, i])))\n",
        "    p, _ = np.histogram(x_train[:, i], bins=bins, density=True)\n",
        "    q, _ = np.histogram(x_test[:, i], bins=bins, density=True)\n",
        "    m = 0.5 * (p + q)\n",
        "    js_divergences[i] = 0.5 * (entropy(p, m) + entropy(q, m))\n",
        "\n",
        "avg_js_divergence = np.mean(js_divergences)\n",
        "print(\"Average Jensen-Shannon divergence:\", avg_js_divergence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ny_Bi-II0m1p",
        "outputId": "8c7bdc82-2397-40dd-8fe4-d2ecfa9bdca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Jensen-Shannon divergence: 0.09482576054076818\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWiTimhNzepZ"
      },
      "source": [
        "### main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20ahgYf1Sccv"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(padded_x, padded_y, test_size=(1 - 0.7), random_state=42)\n",
        "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MPw5g0WU2Py"
      },
      "source": [
        "#### FCN embedded"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm1 = tf.keras.layers.LSTM(hidden_dim, return_sequences=True)\n",
        "        self.dropout = tf.keras.layers.Dropout(0.5)  # Example dropout rate\n",
        "        self.lstm2 = tf.keras.layers.LSTM(hidden_dim)\n",
        "        self.fc = tf.keras.layers.Dense(output_dim, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        embedded = self.embedding(inputs)\n",
        "        lstm_out1 = self.lstm1(embedded)\n",
        "        lstm_out2 = self.lstm2(self.dropout(lstm_out1))\n",
        "        output = self.fc(lstm_out2)\n",
        "        return output"
      ],
      "metadata": {
        "id": "G2gC_WCWPOlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 100\n",
        "hidden_dim = 128\n",
        "output_dim = 25"
      ],
      "metadata": {
        "id": "aNhWLAOLPd4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0loC6vSapfb",
        "outputId": "652b805a-1200-4ad7-878c-de21f57ced34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "516/516 [==============================] - 85s 145ms/step - loss: 1.7678 - accuracy: 0.3410 - val_loss: 0.8555 - val_accuracy: 0.6718\n",
            "Epoch 2/3\n",
            "516/516 [==============================] - 74s 143ms/step - loss: 0.5874 - accuracy: 0.7484 - val_loss: 0.2878 - val_accuracy: 0.8742\n",
            "Epoch 3/3\n",
            "516/516 [==============================] - 71s 138ms/step - loss: 0.1523 - accuracy: 0.9523 - val_loss: 0.0161 - val_accuracy: 0.9995\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7cbc0430e980>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "model = LSTMModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=3, batch_size=20, validation_data=(x_val, y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### predict"
      ],
      "metadata": {
        "id": "CDlOLY-ZXrTi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkYl_HJYqPwv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9eec81e4-7bf5-4b1f-a01d-d21fdd0abb4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 1s 50ms/step\n"
          ]
        }
      ],
      "source": [
        "predictions = model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51Go2kTiqPsU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class_predictions = np.argmax(predictions, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0ybWx0liL2z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dc1ca64-bdf5-40c5-b290-c5f9fc056c38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(y_test, class_predictions)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "TMpU0rx5IVWt",
        "_qGcIy7bIrDY",
        "pz0McM3_ILOO",
        "hWiTimhNzepZ"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
